aws:
  bucket_name: 'sul-sdr-aws-us-west-2-test' # override in prod

ibm:
  bucket_name: 'sul-sdr-ibm-us-south-1-ia' # override in prod

# named storage roots are in the storage_root_map (see config/settings/xxx.yml for examples).
# the storage_root_map contains lookups of storage roots per host.
# see sul-dlss/shared_configs for the storage_root_map of all hosts we deploy to.

moab:
  # storage_trunk is the name of the directory under a storage_root which contains
  # the druid trees:  e.g. 'spec/fixtures/storage_root01/sdr2objects' will contain all the druid
  # trees for this configuration.  if there are multiple storage roots, each will have
  # the subdirectory specified by the same storage_trunk (e.g. 'storage_root1/storage_trunk',
  # 'storage_root2/storage_trunk', etc).
  storage_trunk: 'sdr2objects'
  path_method: druid_tree
  allow_content_subdirs: true

provlog:
  enable: false

storage_root_map: # empty here, override in #{RAILS_ENV}.yml
  default: {}

workflow_services_url: 'https://workflows.example.org/workflow/'
c2m_sql_limit: 1000

checksum_algos: ['md5'] # 'sha1' 'sha256'

zip_storage: '/tmp' # override in #{RAILS_ENV}.yml

resque_dashboard_hostnames: # tells the router where to mount the resque dashboard
  - 'worker-hostname-01.example.com'
  - 'worker-hostname-02.example.com'

# When the backlog of unreplicated moabs is very large, e.g. when first spinning up
# the catalog, or when adding a new ZipEndpoint after the catalog has been running, we
# want to manually choose batches of moabs to replicate, so that we don't accidentally
# overrun the zip creation temp space.
# In normal steady state operation, this should be set to true, either here, or in the
# instance specific configs.
replication:
  audit_should_backfill: false
